"""
Sistema para Entrenar Modelo con Datos de M√∫ltiples Empresas
Maneja diferentes escalas y normaliza adecuadamente
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, RobustScaler
from anomaly_detection import AnomalyDetector
from mejorar_modelo import ModelTrainer
import os
from glob import glob
import json

class MultiCompanyTrainer:
    """
    Sistema para entrenar modelos con datos de m√∫ltiples empresas
    """
    
    def __init__(self, normalization_method='global'):
        """
        Inicializa el entrenador multi-empresa
        
        Parameters:
        -----------
        normalization_method : str
            'global': Normaliza todos los datos juntos
            'per_company': Normaliza por empresa (mantiene escalas relativas)
            'robust': Usa RobustScaler (m√°s resistente a outliers)
        """
        self.normalization_method = normalization_method
        self.companies_info = {}
        self.scaler = None
        self.company_scalers = {}
        self.trainer = ModelTrainer()
    
    def load_company_data(self, filepath, company_name=None, skip_header=True):
        """
        Carga datos de una empresa
        
        Parameters:
        -----------
        filepath : str
            Ruta al archivo CSV
        company_name : str, optional
            Nombre de la empresa (si no se infiere del nombre del archivo)
        skip_header : bool
            Si saltar fila de unidades
            
        Returns:
        --------
        pandas.DataFrame : Datos de la empresa con columna 'Empresa'
        """
        try:
            # Inferir nombre de empresa del archivo si no se proporciona
            if company_name is None:
                company_name = os.path.splitext(os.path.basename(filepath))[0]
                # Limpiar nombre (remover prefijos comunes)
                company_name = company_name.replace('datos_energia_', '').replace('datos_', '')
            
            # Cargar datos
            skiprows = [1] if skip_header and self._has_header_row(filepath) else None
            df = pd.read_csv(filepath, encoding='utf-8', skiprows=skiprows)
            df.columns = df.columns.str.strip()
            
            # Convertir num√©ricas
            numeric_columns = [
                'Generaci√≥n total', 'Consumo total', 'Autoconsumo',
                'Energ√≠a suministrada a la red', 'Energ√≠a obtenida de la red'
            ]
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Agregar columna de empresa
            df['Empresa'] = company_name
            
            # Guardar info de la empresa
            self.companies_info[company_name] = {
                'file': os.path.basename(filepath),
                'records': len(df),
                'mean_generation': df['Generaci√≥n total'].mean() if 'Generaci√≥n total' in df.columns else 0,
                'mean_consumption': df['Consumo total'].mean() if 'Consumo total' in df.columns else 0,
                'std_generation': df['Generaci√≥n total'].std() if 'Generaci√≥n total' in df.columns else 0,
                'std_consumption': df['Consumo total'].std() if 'Consumo total' in df.columns else 0
            }
            
            print(f"‚úÖ {company_name}: {len(df)} registros cargados")
            print(f"   Generaci√≥n promedio: {self.companies_info[company_name]['mean_generation']:.2f} Wh")
            print(f"   Consumo promedio: {self.companies_info[company_name]['mean_consumption']:.2f} Wh")
            
            return df
            
        except Exception as e:
            print(f"‚ùå Error cargando {filepath}: {e}")
            return None
    
    def _has_header_row(self, filepath):
        """Verifica si tiene fila de unidades"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                second_line = f.readline().strip()
                return '[' in second_line and ']' in second_line
        except:
            return False
    
    def combine_companies(self, company_dataframes):
        """
        Combina datos de m√∫ltiples empresas
        
        Parameters:
        -----------
        company_dataframes : list
            Lista de DataFrames, cada uno con columna 'Empresa'
            
        Returns:
        --------
        pandas.DataFrame : Datos combinados
        """
        if not company_dataframes:
            return None
        
        print(f"\nüîÑ Combinando datos de {len(company_dataframes)} empresas...")
        
        # Combinar
        combined_df = pd.concat(company_dataframes, ignore_index=True)
        
        print(f"‚úÖ Total registros combinados: {len(combined_df)}")
        print(f"\nüìä Distribuci√≥n por empresa:")
        company_counts = combined_df['Empresa'].value_counts()
        for company, count in company_counts.items():
            print(f"   {company}: {count} registros ({count/len(combined_df)*100:.1f}%)")
        
        return combined_df
    
    def normalize_data(self, df, method=None):
        """
        Normaliza datos seg√∫n el m√©todo seleccionado
        
        Parameters:
        -----------
        df : pandas.DataFrame
            Datos a normalizar
        method : str, optional
            M√©todo de normalizaci√≥n (usa self.normalization_method si es None)
            
        Returns:
        --------
        pandas.DataFrame : Datos normalizados
        """
        method = method or self.normalization_method
        
        print(f"\n‚öôÔ∏è Normalizando datos (m√©todo: {method})...")
        
        numeric_columns = [
            'Generaci√≥n total', 'Consumo total', 'Autoconsumo',
            'Energ√≠a suministrada a la red', 'Energ√≠a obtenida de la red'
        ]
        
        df_normalized = df.copy()
        
        if method == 'global':
            # Normalizaci√≥n global: todos los datos juntos
            print("   Normalizando todos los datos juntos (escala global)...")
            self.scaler = StandardScaler()
            
            for col in numeric_columns:
                if col in df.columns:
                    values = df[col].values.reshape(-1, 1)
                    df_normalized[col] = self.scaler.fit_transform(values).flatten()
            
        elif method == 'per_company':
            # Normalizaci√≥n por empresa: mantiene escalas relativas
            print("   Normalizando por empresa (mantiene escalas relativas)...")
            
            for company in df['Empresa'].unique():
                company_mask = df['Empresa'] == company
                company_data = df[company_mask]
                
                if company not in self.company_scalers:
                    self.company_scalers[company] = StandardScaler()
                
                for col in numeric_columns:
                    if col in df.columns:
                        values = company_data[col].values.reshape(-1, 1)
                        normalized = self.company_scalers[company].fit_transform(values).flatten()
                        df_normalized.loc[company_mask, col] = normalized
                
                print(f"   ‚úÖ {company}: normalizado")
        
        elif method == 'robust':
            # RobustScaler: m√°s resistente a outliers
            print("   Usando RobustScaler (resistente a outliers)...")
            self.scaler = RobustScaler()
            
            for col in numeric_columns:
                if col in df.columns:
                    values = df[col].values.reshape(-1, 1)
                    df_normalized[col] = self.scaler.fit_transform(values).flatten()
        
        else:
            print(f"‚ö†Ô∏è M√©todo desconocido: {method}, usando global")
            return self.normalize_data(df, method='global')
        
        print("‚úÖ Normalizaci√≥n completada")
        
        return df_normalized
    
    def prepare_features_multi_company(self, df):
        """
        Prepara caracter√≠sticas incluyendo informaci√≥n de empresa
        
        Parameters:
        -----------
        df : pandas.DataFrame
            Datos con columna 'Empresa'
            
        Returns:
        --------
        pandas.DataFrame : Caracter√≠sticas preparadas
        """
        # Usar el m√©todo de prepare_features pero adaptado
        from anomaly_detection import AnomalyDetector
        temp_detector = AnomalyDetector()
        
        # Preparar caracter√≠sticas base
        X_base, _ = temp_detector.prepare_features(df)
        
        # Agregar caracter√≠sticas de empresa (encoding)
        if 'Empresa' in df.columns:
            # One-hot encoding de empresa
            company_dummies = pd.get_dummies(df['Empresa'], prefix='Empresa')
            X_base = pd.concat([X_base, company_dummies], axis=1)
        
        return X_base, df
    
    def train_multi_company(self, company_files, contamination=0.1, 
                           normalization_method=None, save_model='modelo_multi_empresa.pkl'):
        """
        Entrena modelo con datos de m√∫ltiples empresas
        
        Parameters:
        -----------
        company_files : list
            Lista de rutas a archivos CSV de diferentes empresas
        contamination : float
            Valor de contamination
        normalization_method : str, optional
            M√©todo de normalizaci√≥n
        save_model : str
            Nombre del modelo a guardar
            
        Returns:
        --------
        AnomalyDetector : Modelo entrenado
        """
        print("=" * 60)
        print("üè¢ ENTRENAMIENTO CON M√öLTIPLES EMPRESAS")
        print("=" * 60)
        
        if normalization_method:
            self.normalization_method = normalization_method
        
        # 1. Cargar datos de todas las empresas
        print(f"\nüìÇ Cargando datos de {len(company_files)} empresas...")
        company_dfs = []
        
        for filepath in company_files:
            df = self.load_company_data(filepath)
            if df is not None:
                company_dfs.append(df)
        
        if not company_dfs:
            print("‚ùå No se pudieron cargar datos de empresas")
            return None
        
        # 2. Combinar
        combined_df = self.combine_companies(company_dfs)
        
        # 3. Normalizar
        normalized_df = self.normalize_data(combined_df)
        
        # 4. Preparar caracter√≠sticas
        print(f"\nüîß Preparando caracter√≠sticas...")
        X, df_processed = self.prepare_features_multi_company(normalized_df)
        
        print(f"‚úÖ Caracter√≠sticas preparadas: {X.shape[1]} caracter√≠sticas")
        print(f"   Incluye: {len([c for c in X.columns if c.startswith('Empresa_')])} empresas")
        
        # 5. Entrenar modelo
        print(f"\nüéØ Entrenando Isolation Forest...")
        print(f"   Contamination: {contamination}")
        print(f"   Registros: {len(X)}")
        print(f"   Caracter√≠sticas: {X.shape[1]}")
        
        # Crear detector personalizado
        detector = MultiCompanyAnomalyDetector(
            contamination=contamination,
            normalization_method=self.normalization_method,
            scaler=self.scaler,
            company_scalers=self.company_scalers,
            companies_info=self.companies_info
        )
        
        # Normalizar caracter√≠sticas finales (todas las caracter√≠sticas preparadas)
        feature_scaler = StandardScaler()
        X_scaled = feature_scaler.fit_transform(X)
        
        # Guardar el scaler de caracter√≠sticas (no el de columnas individuales)
        detector.feature_scaler = feature_scaler
        
        # Entrenar
        detector.model.fit(X_scaled)
        detector.is_fitted = True
        detector.feature_names = X.columns.tolist()
        detector.companies = list(self.companies_info.keys())
        
        # Guardar
        self.trainer.save_model(detector, save_model)
        
        # Guardar info de empresas
        info_file = save_model.replace('.pkl', '_empresas.json')
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(self.companies_info, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\n‚úÖ Modelo entrenado y guardado:")
        print(f"   üìÅ Modelo: {save_model}")
        print(f"   üìÅ Info empresas: {info_file}")
        print(f"   üè¢ Empresas: {len(self.companies_info)}")
        
        return detector
    
    def compare_normalization_methods(self, company_files, contamination=0.1):
        """
        Compara diferentes m√©todos de normalizaci√≥n
        
        Parameters:
        -----------
        company_files : list
            Lista de archivos de empresas
        contamination : float
            Valor de contamination
        """
        print("=" * 60)
        print("‚öñÔ∏è COMPARACI√ìN DE M√âTODOS DE NORMALIZACI√ìN")
        print("=" * 60)
        
        methods = ['global', 'per_company', 'robust']
        results = {}
        
        for method in methods:
            print(f"\n{'='*60}")
            print(f"üîç Probando m√©todo: {method}")
            print(f"{'='*60}")
            
            trainer = MultiCompanyTrainer(normalization_method=method)
            detector = trainer.train_multi_company(
                company_files, 
                contamination=contamination,
                save_model=f'modelo_test_{method}.pkl'
            )
            
            if detector:
                # Evaluar (cargar datos y predecir)
                company_dfs = []
                for filepath in company_files:
                    df = trainer.load_company_data(filepath)
                    if df is not None:
                        company_dfs.append(df)
                
                if company_dfs:
                    combined = trainer.combine_companies(company_dfs)
                    normalized = trainer.normalize_data(combined)
                    results_df = detector.get_anomalies(normalized)
                    
                    num_anomalies = results_df['Es_Anomalia'].sum()
                    results[method] = {
                        'anomalies': num_anomalies,
                        'percentage': num_anomalies / len(results_df) * 100
                    }
        
        # Mostrar comparaci√≥n
        print(f"\n{'='*60}")
        print("üìä RESULTADOS DE COMPARACI√ìN")
        print(f"{'='*60}")
        
        for method, result in results.items():
            print(f"\n{method.upper()}:")
            print(f"   Anomal√≠as: {result['anomalies']}")
            print(f"   Porcentaje: {result['percentage']:.2f}%")
        
        # Recomendaci√≥n
        print(f"\nüí° RECOMENDACI√ìN:")
        # El m√©todo con separaci√≥n m√°s clara ser√≠a mejor, pero necesitar√≠amos evaluar
        print("   - 'global': Mejor si las empresas tienen escalas similares")
        print("   - 'per_company': Mejor si las empresas tienen escalas muy diferentes")
        print("   - 'robust': Mejor si hay muchos outliers")
        
        return results


class MultiCompanyAnomalyDetector(AnomalyDetector):
    """
    Detector de anomal√≠as adaptado para m√∫ltiples empresas
    """
    
    def __init__(self, contamination=0.1, normalization_method='global',
                 scaler=None, company_scalers=None, companies_info=None):
        super().__init__(contamination=contamination)
        self.normalization_method = normalization_method
        self.scaler = scaler  # Scaler para columnas num√©ricas originales
        self.company_scalers = company_scalers or {}
        self.companies_info = companies_info or {}
        self.companies = []
        self.feature_scaler = None  # Scaler para caracter√≠sticas finales preparadas
    
    def prepare_features(self, df):
        """Prepara caracter√≠sticas para m√∫ltiples empresas"""
        from anomaly_detection import AnomalyDetector
        temp_detector = AnomalyDetector()
        
        # Preparar caracter√≠sticas base
        X_base, df_processed = temp_detector.prepare_features(df)
        
        # Agregar encoding de empresa si existe
        if 'Empresa' in df.columns:
            company_dummies = pd.get_dummies(df['Empresa'], prefix='Empresa')
            # Asegurar que todas las empresas del entrenamiento est√©n presentes
            for company in self.companies:
                col_name = f'Empresa_{company}'
                if col_name not in company_dummies.columns:
                    company_dummies[col_name] = 0
            
            # Mantener solo empresas conocidas
            known_company_cols = [c for c in company_dummies.columns 
                                if any(c.endswith(f'_{comp}') for comp in self.companies)]
            company_dummies = company_dummies[known_company_cols]
            
            X_base = pd.concat([X_base, company_dummies], axis=1)
        
        return X_base, df_processed
    
    def predict(self, df):
        """Predice con normalizaci√≥n adecuada"""
        if not self.is_fitted:
            raise ValueError("Modelo no entrenado")
        
        # Normalizar datos seg√∫n m√©todo
        df_normalized = df.copy()
        
        numeric_columns = [
            'Generaci√≥n total', 'Consumo total', 'Autoconsumo',
            'Energ√≠a suministrada a la red', 'Energ√≠a obtenida de la red'
        ]
        
        if self.normalization_method == 'global' and self.scaler:
            for col in numeric_columns:
                if col in df.columns:
                    values = df[col].values.reshape(-1, 1)
                    df_normalized[col] = self.scaler.transform(values).flatten()
        
        elif self.normalization_method == 'per_company':
            for company in df['Empresa'].unique():
                company_mask = df['Empresa'] == company
                if company in self.company_scalers:
                    for col in numeric_columns:
                        if col in df.columns:
                            values = df.loc[company_mask, col].values.reshape(-1, 1)
                            df_normalized.loc[company_mask, col] = \
                                self.company_scalers[company].transform(values).flatten()
        
        # Preparar caracter√≠sticas y predecir
        X, _ = self.prepare_features(df_normalized)
        X = X[self.feature_names]
        
        # Usar el scaler de caracter√≠sticas que se us√≥ en entrenamiento
        if self.feature_scaler is not None:
            X_scaled = self.feature_scaler.transform(X)
        else:
            # Fallback: crear nuevo scaler si no existe
            X_scaled = StandardScaler().fit_transform(X)
        
        return self.model.predict(X_scaled)
    
    def predict_proba(self, df):
        """Obtiene scores de anomal√≠a con normalizaci√≥n adecuada"""
        if not self.is_fitted:
            raise ValueError("El modelo debe ser entrenado primero")
        
        # Normalizar datos seg√∫n m√©todo
        df_normalized = df.copy()
        
        numeric_columns = [
            'Generaci√≥n total', 'Consumo total', 'Autoconsumo',
            'Energ√≠a suministrada a la red', 'Energ√≠a obtenida de la red'
        ]
        
        if self.normalization_method == 'global' and self.scaler:
            for col in numeric_columns:
                if col in df.columns:
                    values = df[col].values.reshape(-1, 1)
                    df_normalized[col] = self.scaler.transform(values).flatten()
        
        elif self.normalization_method == 'per_company':
            for company in df['Empresa'].unique():
                company_mask = df['Empresa'] == company
                if company in self.company_scalers:
                    for col in numeric_columns:
                        if col in df.columns:
                            values = df.loc[company_mask, col].values.reshape(-1, 1)
                            df_normalized.loc[company_mask, col] = \
                                self.company_scalers[company].transform(values).flatten()
        
        # Preparar caracter√≠sticas
        X, _ = self.prepare_features(df_normalized)
        X = X[self.feature_names]
        
        # Usar el scaler de caracter√≠sticas que se us√≥ en entrenamiento
        if self.feature_scaler is not None:
            X_scaled = self.feature_scaler.transform(X)
        else:
            # Fallback: crear nuevo scaler si no existe
            X_scaled = StandardScaler().fit_transform(X)
        
        # Obtener scores
        scores = self.model.score_samples(X_scaled)
        
        return scores


def main():
    """Funci√≥n principal"""
    print("=" * 60)
    print("üè¢ ENTRENAMIENTO CON M√öLTIPLES EMPRESAS")
    print("=" * 60)
    
    # Buscar archivos de empresas
    print("\nüîç Buscando archivos de empresas...")
    company_files = glob('datos_energia*.csv') + glob('empresa_*.csv') + glob('*_energia.csv')
    
    if not company_files:
        print("‚ö†Ô∏è No se encontraron archivos autom√°ticamente")
        print("üí° Proporciona manualmente las rutas:")
        company_files = []
        while True:
            filepath = input("Ruta al archivo (Enter para terminar): ").strip()
            if not filepath:
                break
            if os.path.exists(filepath):
                company_files.append(filepath)
            else:
                print(f"‚ö†Ô∏è Archivo no encontrado: {filepath}")
    else:
        print(f"‚úÖ Encontrados {len(company_files)} archivos:")
        for f in company_files:
            print(f"   - {os.path.basename(f)}")
    
    if not company_files:
        print("‚ùå No hay archivos para procesar")
        return
    
    # Men√∫
    print("\n" + "=" * 60)
    print("¬øQu√© deseas hacer?")
    print("1. Entrenar con normalizaci√≥n global (recomendado)")
    print("2. Entrenar con normalizaci√≥n por empresa")
    print("3. Entrenar con RobustScaler")
    print("4. Comparar todos los m√©todos")
    
    opcion = input("\nSelecciona (1-4): ").strip()
    contamination = float(input("Contamination (default 0.05): ").strip() or "0.05")
    
    trainer = MultiCompanyTrainer()
    
    if opcion == '1':
        detector = trainer.train_multi_company(
            company_files, 
            contamination=contamination,
            normalization_method='global',
            save_model='modelo_multi_empresa_global.pkl'
        )
    elif opcion == '2':
        detector = trainer.train_multi_company(
            company_files,
            contamination=contamination,
            normalization_method='per_company',
            save_model='modelo_multi_empresa_por_empresa.pkl'
        )
    elif opcion == '3':
        detector = trainer.train_multi_company(
            company_files,
            contamination=contamination,
            normalization_method='robust',
            save_model='modelo_multi_empresa_robust.pkl'
        )
    elif opcion == '4':
        trainer.compare_normalization_methods(company_files, contamination)
    else:
        print("‚ùå Opci√≥n no v√°lida")


if __name__ == "__main__":
    main()

